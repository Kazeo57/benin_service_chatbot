# -*- coding: utf-8 -*-
"""Chatbot_memory.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15aQDuABIfqh-yY87u9JiH4u4XHr8y7EQ
"""

#!pip install langchain langchain_community colorama

"""#Process data"""

#!pip install jq





#!pip install langchain_google_genai




"""#Database"""

#!pip install langchain-chroma chroma

#!pip install google-generativeai

from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
#import google.generativeai as genai
#from chromadb import Documents, EmbeddingFunction, Embeddings
from google.api_core import retry
from langchain_chroma import Chroma
#from google.colab import userdata
#from IPython.display import Markdown
from langchain.chains import RetrievalQA, ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from langchain import PromptTemplate
#from langchain.retrievers import TFIDFRetriever
from chromadb import PersistentClient
import os
#import chromadb
from dotenv import load_dotenv
load_dotenv()
GOOGLE_API_KEY=os.getenv('google_api_key')
#genai.configure(api_key=GOOGLE_API_KEY)
embeddings=GoogleGenerativeAIEmbeddings(google_api_key=GOOGLE_API_KEY,model="models/text-embedding-004")
"""
vector_store=Chroma(
    collection_name="online_service_22_02_2025",
    embedding_function=embeddings,
    persist_directory="./vector_db"
)
vector_store.add()
class GeminiEmbeddingFunction(EmbeddingFunction):
    # Specify whether to generate embeddings for documents, or queries
    document_mode = True

    def __call__(self, input: Documents) -> Embeddings:
        if self.document_mode:
            embedding_task = "retrieval_document"
        else:
            embedding_task = "retrieval_query"

        retry_policy = {"retry": retry.Retry(predicate=retry.if_transient_error)}

        response = genai.embed_content(
            model="models/text-embedding-004",
            content=input,
            task_type=embedding_task,
            request_options=retry_policy,
        )
        return response["embedding"]"""

embeddings

#splits=text_splitter.split_documents(documents)



vector_store_loaded=Chroma(
    persist_directory="E:/NLP practice/services_db",
    collection_name='online_service_22_02_2025',
    embedding_function=embeddings,
)


print("Listdir",os.listdir("services_db"))

client=PersistentClient(path="services_db")
print("liste Collection",client.list_collections())
print("Collection",vector_store_loaded._collection.count())

retriever=vector_store_loaded.as_retriever(
    #task_type="semantic_similarity",
    search_type="mmr",#"similarity",#
    search_kwargs={"k":15}
)

#query="Quelles sont les démarches pour avoir mon CIP?"
#docs=retriever.get_relevant_documents(query)   

#for doc in docs:
  #print("Contenu",doc.page_content)
  #print("Metadata",doc.metadata)

#docs




#from langchain.retrievers import InMemoryRetriever
#splits=text_splitter.split_documents(documents)
#retriever=TFIDFRetriever.from_documents(splits)

template = """Tu es GBOT ,un assistant pour les services publics au Bénin.
Utilise le contexte (delimited by <ctx></ctx>) et l'historique (delimited by <hs></hs>) pour répondre aux péoccupation de l'utilisateur.Fais de ton mieux pour l'aider et fouurni toujours le lien vers le service à la fin:
------
<ctx>
{context}
</ctx>
------
<hs>
{history}
</hs>
------
{question}
Answer:
"""
prompt = PromptTemplate(
    input_variables=["history", "context", "question"],
    template=template,
)

#GOOGLE_API_KEY=os.environ.get('google_api_key')
#print("KEY",GOOGLE_API_KEY)
#print("Key",GOOGLE_API_KEY)
#embeddings=GoogleGenerativeAIEmbeddings(google_api_key=GOOGLE_API_KEY,model="models/text-embedding-004")

qa = RetrievalQA.from_chain_type(
    llm=ChatGoogleGenerativeAI(google_api_key=GOOGLE_API_KEY,model='gemini-1.5-flash',temperature=1),#google_api_key=),
    chain_type='stuff',
    retriever=retriever,
    verbose=True,
    chain_type_kwargs={
        "verbose": True,
        "prompt": prompt,
        "memory": ConversationBufferMemory(
            memory_key="history",
            input_key="question"),
    }
)

#retriever

#qa.run({"query":"Comment avoir mon passeport"})
#Markdown(qa.run({"query":"Quelles sont les démarches pour avoir mon CIP?"}))

def chat(query="Je voudrais un passeport ordinaire,je suis Béninois résident au Bénin"):
    output=qa.invoke(input=query)
    response=output.get('result')
    return response

#print(chat())




